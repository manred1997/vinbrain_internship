{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from colorama import Fore\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "slow_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('bert-base-uncased\\\\tokenizer_config.json',\n",
       " 'bert-base-uncased\\\\special_tokens_map.json',\n",
       " 'bert-base-uncased\\\\vocab.txt',\n",
       " 'bert-base-uncased\\\\added_tokens.json')"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "slow_tokenizer.save_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "\n",
    "# tokenizer = BertWordPieceTokenizer(\"week2/tokenzier/vocab.txt\", lowercase=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# slow_tokenizer.save_pretrained(\"bert_base_uncased/\")\n",
    "tokenizer = BertWordPieceTokenizer(\"bert-base-uncased/vocab.txt\", lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "slow_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=30522, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=True, wordpieces_prefix=##)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"AAAI-21-SDU-shared-task-2-AD-master/dataset/train.json\") as f:\n",
    "    train = json.load(f)\n",
    "with open(\"AAAI-21-SDU-shared-task-2-AD-master/dataset/diction.json\") as f:\n",
    "    diction = json.load(f)\n",
    "with open(\"AAAI-21-SDU-shared-task-2-AD-master/dataset/dev.json\") as f:\n",
    "    dev = json.load(f)\n",
    "with open(\"AAAI-21-SDU-shared-task-2-AD-master/dataset/predictions.json\") as f:\n",
    "    predictions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "restrial base station'],\n",
       " 'SEP': ['separator', 'symbol error probability'],\n",
       " 'BR': ['boundary refinement',\n",
       "  'binary relevance',\n",
       "  'bug reports',\n",
       "  'belief revision',\n",
       "  'best response',\n",
       "  'bone region'],\n",
       " 'RWA': ['random walker algorithm',\n",
       "  'right wing authoritarianism',\n",
       "  'recurrent weighted average'],\n",
       " 'DCI': ['downlink control information', 'downlink control indicator'],\n",
       " 'RE': ['resource elements',\n",
       "  'relation extraction',\n",
       "  'renewable energy',\n",
       "  'requirements elicitation',\n",
       "  'referring expression'],\n",
       " 'PDF': ['probability density function',\n",
       "  'portable document format',\n",
       "  'primary distribution format'],\n",
       " 'PAT': ['perform adversarial training', 'process arrival time'],\n",
       " 'SIC': ['successive interference cancellation',\n",
       "  'static induction control',\n",
       "  'self interference cancellation'],\n",
       " 'DFA': ['direct feedback alignment', 'deterministic finite automaton'],\n",
       " 'IEC': ['information embedding cost',\n",
       "  'international electrotechnical commission'],\n",
       " 'IAN': ['introspective adversarial network', 'interference as noise'],\n",
       " 'WN': ['weak normalization', 'weight normalization'],\n",
       " 'TTP': ['trusted third party', 'total transmit power'],\n",
       " 'SSM': ['state space model', 'statistical shape modeling'],\n",
       " 'WSI': ['whole slide image', 'word sense induction'],\n",
       " 'CDA': ['concurrent dialogue acts',\n",
       "  'christen democratisch appel',\n",
       "  'canonical discriminant analysis',\n",
       "  'continuous decomposition analysis'],\n",
       " 'BL': ['black level subtraction', 'bayesian learning'],\n",
       " 'NSS': ['normalized scan - path saliency', 'non - local self similar'],\n",
       " 'VSI': ['virtual switch instances',\n",
       "  'variational system identification',\n",
       "  'voltage source inverter'],\n",
       " 'RRC': ['rank residual constraint', 'radio resource control'],\n",
       " 'ASF': ['apache software foundation', 'african swine fever'],\n",
       " 'AKS': ['almost known sets', 'asimmetric kernel scaling'],\n",
       " 'ICE': ['intrinsic control error', 'interactive connectivity establishment'],\n",
       " 'PDP': ['partial dependence plots',\n",
       "  'product display page',\n",
       "  'policy decision point'],\n",
       " 'RD': ['real data',\n",
       "  'residual denoiser',\n",
       "  'research and development',\n",
       "  'relative difference',\n",
       "  'reciprocal degree'],\n",
       " 'IT': ['iris thickness',\n",
       "  'immediate threshold',\n",
       "  'inferior temporal',\n",
       "  'image translation'],\n",
       " 'CBP': ['coprime blur pairs', 'compact bilinear pooling'],\n",
       " 'RFS': ['random finite set', 'rain fog snow'],\n",
       " 'IFD': ['indian face database', 'icelandic frequency dictionary'],\n",
       " 'CDR': ['call detail records',\n",
       "  'critical design review',\n",
       "  'clock difference relations'],\n",
       " 'DBP': ['discrete base problem',\n",
       "  'determinisable by pruning',\n",
       "  'digital back propagation'],\n",
       " 'BLE': ['bluetooth low energy', 'bilingual lexicon extraction'],\n",
       " 'RTF': ['region templates framework', 'real time factor'],\n",
       " 'BSP': ['binary space partitioning', 'bulk synchronous parallel'],\n",
       " 'RCA': ['root certificate authority',\n",
       "  'ripple carry adder',\n",
       "  'root cause analysis',\n",
       "  'reverse classification accuracy'],\n",
       " 'LBP': ['local binary pattern', 'loopy belief propagation'],\n",
       " 'LML': ['lifelong metric learning',\n",
       "  'log marginal likelihood',\n",
       "  'lifelong machine learning'],\n",
       " 'ADF': ['anisotropic diffusion filter', 'automatically defined function'],\n",
       " 'DML': ['data modification layer', 'declarative ml language'],\n",
       " 'BQ': ['basic question', 'bayesian quadrature'],\n",
       " 'LA': ['logical access', 'layout analysis', 'left atrium', 'location area'],\n",
       " 'NPR': ['near perfect reconstruction', 'normalized probabilistic rand'],\n",
       " 'PEP': ['permission enforcement point', 'policy enforcement point'],\n",
       " 'PPT': ['pignistic probability transformation',\n",
       "  'privacy preserving techniques'],\n",
       " 'DMA': ['direct memory access',\n",
       "  'dynamic mechanical analysis',\n",
       "  'data market austria'],\n",
       " 'SDF': ['side - stream dark field',\n",
       "  'signed distance function',\n",
       "  'signed distance field'],\n",
       " 'SU': ['symmetric uncertainty', 'secondary user'],\n",
       " 'RPG': ['relevance proximity graph', 'robust principal graph'],\n",
       " 'PIT': ['permutation invariant training', 'pending interest table'],\n",
       " 'CB': ['code block',\n",
       "  'content - based',\n",
       "  'circular buffered',\n",
       "  'compression benchmark',\n",
       "  'causal box'],\n",
       " 'DMF': ['dynamic mode factorization', 'drone - cell management frame'],\n",
       " 'DBA': ['dtw barycenter averaging', 'deterministic buchi automaton'],\n",
       " 'SRL': ['semantic role labeling',\n",
       "  'state representation learning',\n",
       "  'statistical relational learning'],\n",
       " 'RQ': ['research question', 'reformulated queries'],\n",
       " 'SBM': ['stochastic block model',\n",
       "  'sequential monte carlo',\n",
       "  'standard bit mutations',\n",
       "  'shape boltzmann machine'],\n",
       " 'FJ': ['friendly jamming', 'featherweight java'],\n",
       " 'SPF': ['structure propagation fusion', 'shortest path forest'],\n",
       " 'HM': ['harmonic mean', 'hybrid model'],\n",
       " 'LOD': ['linked open data', 'level of detail'],\n",
       " 'GTD': ['grasp type detection', 'grasp type dataset'],\n",
       " 'TCR': ['trap control register', 'transductive cascaded regression'],\n",
       " 'LE': ['low energy', 'label equivalence'],\n",
       " 'LCS': ['longest common subsequence', 'local causal states'],\n",
       " 'FAST': ['features from accelerated segment test',\n",
       "  'file and storage technologies'],\n",
       " 'SSE': ['streaming simd extensions', 'spherical semantic embedding'],\n",
       " 'DSR': ['dynamic sparse reparameterization', 'dynamic source routing'],\n",
       " 'GPM': ['graph pattern matching', 'matchinggraph pattern matching'],\n",
       " 'VR': ['virtual reality', 'visibility region', 'vigilance reward'],\n",
       " 'MPA': ['music performance analysis', 'message passing algorithm'],\n",
       " 'SAN': ['semantic alignment network',\n",
       "  'saturation analysis',\n",
       "  'subject alternate name',\n",
       "  'stacked attention network',\n",
       "  'self attention network'],\n",
       " 'SSI': ['subspace system identification',\n",
       "  'social system identification',\n",
       "  'software sustainability institute'],\n",
       " 'TDM': ['technical debt management',\n",
       "  'temporal difference model',\n",
       "  'time division multiplexing'],\n",
       " 'NS': ['network science', 'negative sampling', 'neutron star'],\n",
       " 'SUs': ['secondary users', 'spectrum usage'],\n",
       " 'PBS': ['primary base station', 'public broadcasting service'],\n",
       " 'PCL': ['positive coalgebraic logics',\n",
       "  'point cloud library',\n",
       "  'path consistency learning'],\n",
       " 'DMN': ['dynamic memory network', 'default mode network'],\n",
       " 'SFM': ['social force model',\n",
       "  'structural factorization machine',\n",
       "  'structure from motion'],\n",
       " 'GF': ['guided filtering', 'gabor filter'],\n",
       " 'CDP': ['centralized differential privacy', 'classical dynamic programming'],\n",
       " 'SAM': ['semi - autonomous machine',\n",
       "  'speaker - addressee model',\n",
       "  'search of associative memory',\n",
       "  'self - assessment manikin'],\n",
       " 'TM': ['tone mapping', 'teacher mark', 'turing machine'],\n",
       " 'DADA': ['distributed affinity dual approximation',\n",
       "  'dual adversarial domain adaptation'],\n",
       " 'DEC': ['deep embedded clustering', 'dense - captioning event'],\n",
       " 'CNL': ['controlled natural language', 'certain natural language'],\n",
       " 'PIN': ['proposal indexing network', 'phrase indexing network'],\n",
       " 'USD': ['unmet system demand', 'unambiguous state discrimination'],\n",
       " 'HC': ['healthy control', 'hill - climbing', 'hierarchical classification'],\n",
       " 'GSP': ['generalized second price', 'global statistics pooling'],\n",
       " 'ESS': ['effective sample size', 'evolutionary stable strategies'],\n",
       " 'LDS': ['low density spreading', 'linear dynamical system'],\n",
       " 'ARD': ['adversarially robust distillation',\n",
       "  'automatic relevance determination',\n",
       "  'accelerated robust distillation'],\n",
       " 'NL': ['network lifetime', 'natural language'],\n",
       " 'ILS': ['incomplete lineage sorting', 'iterated local search'],\n",
       " 'AML': ['adversarial machine learning', 'actor modeling language'],\n",
       " 'MN': ['mobile network', 'master node', 'memory networks', 'mobile node'],\n",
       " 'SED': ['soft edit distance',\n",
       "  'sound event detection',\n",
       "  'standard edit distance'],\n",
       " 'UDP': ['user datagram protocol', 'universal dependency parse'],\n",
       " 'SSL': ['structural sparsity learning',\n",
       "  'scleral spur location',\n",
       "  'semi supervised learning'],\n",
       " 'SCR': ['scratch',\n",
       "  'sparse compositional regression',\n",
       "  'skin conductance response'],\n",
       " 'CKA': ['concurrent kleene algebra', 'centered kernel alignment'],\n",
       " 'RDS': ['relational database service', 'running digital sum'],\n",
       " 'WF': ['white females', 'weighted fusion'],\n",
       " 'TBB': ['tor browser bundle', 'threading building blocks'],\n",
       " 'DI': ['document index',\n",
       "  'dyadic indicator',\n",
       "  'direct inspection',\n",
       "  'dependency injection'],\n",
       " 'TPD': ['turbo product decoder', 'total project delay'],\n",
       " 'NIC': ['neural image caption', 'network interface card'],\n",
       " 'DSA': ['data science and analytics', 'digital signature algorithm'],\n",
       " 'MRS': ['magnetic resonance spectroscopy', 'multiset rewriting systems'],\n",
       " 'CO': ['context - only attention', 'carbon - oxygen'],\n",
       " 'NSP': ['neural sequence prediction', 'next sentence prediction'],\n",
       " 'FG': ['favoured granted', 'filter gate'],\n",
       " 'BSC': ['binary symmetric channel', 'base station controller'],\n",
       " 'BSD': ['blind spot detection', 'berkeley segmentation dataset'],\n",
       " 'FPS': ['frame per second', 'false projection selection'],\n",
       " 'SPS': ['signal processing systems',\n",
       "  'surcharge pricing scheme',\n",
       "  'signal probability skey'],\n",
       " 'PPE': ['per - pixel - error', 'predictive performance equation'],\n",
       " 'EMS': ['elevated mean scan statistic',\n",
       "  'event management system',\n",
       "  'elevated mean scan'],\n",
       " 'FAR': ['false acceptance rate', 'flow annotation replanning'],\n",
       " 'CFD': ['computational fluid dynamics', 'carrier frequency difference'],\n",
       " 'DSO': ['direct sparse odometry', 'distribution system operator'],\n",
       " 'DTP': ['difference target propagation', 'dynamic trajectory predictor'],\n",
       " 'GI': ['gradient initialization', 'graph isomorphism'],\n",
       " 'SSS': ['staggered sample selection', 'stochastically stable states'],\n",
       " 'DST': ['dialogue state tracker', 'discrete sine transform'],\n",
       " 'ID': ['item description',\n",
       "  'information decoding',\n",
       "  'input data',\n",
       "  'interleaved declustering'],\n",
       " 'FEA': ['factored evolutionary algorithms', 'finite element analysis'],\n",
       " 'SB': ['systems biology', 'symmetry breaking'],\n",
       " 'SG': ['skip gram', 'stochastic gradient'],\n",
       " 'CLT': ['central limit theorem', 'cognitive load theory'],\n",
       " 'MET': ['michigan english test', 'multi - edge type'],\n",
       " 'GSR': ['geographic source routing',\n",
       "  'group sparsity residual',\n",
       "  'group sparse representation'],\n",
       " 'GPR': ['gaussian process regression', 'gamma passing rate'],\n",
       " 'STA': ['static timing analysis', 'super - twisting algorithms'],\n",
       " 'HMC': ['hybrid monte carlo', 'hamiltonian monte carlo'],\n",
       " 'LSM': ['laplacian - based shape matching', 'lock sweeping method'],\n",
       " 'IO': ['inverse optimization',\n",
       "  'iterative optimization',\n",
       "  'interacting object'],\n",
       " 'LAP': ['low altitude platform', 'linear assignment problem'],\n",
       " 'LTP': ['licklider transmission protocol', 'long term potentiation'],\n",
       " 'GMS': ['grid - based motion statistics', 'gaussian material synthesis'],\n",
       " 'DCM': ['dynamics canalization map',\n",
       "  'discontinuous conduction mode',\n",
       "  'deep choice model',\n",
       "  'discrete choice models',\n",
       "  'device configuration manager'],\n",
       " 'MGE': ['minimum generation error', 'multi - granularity embedding'],\n",
       " 'MCS': ['maximal consistent set',\n",
       "  'modulation and coding scheme',\n",
       "  'maximum cardinality search'],\n",
       " 'OP': ['orthogonal procrustes',\n",
       "  'old persian',\n",
       "  'outage probability',\n",
       "  'original precision',\n",
       "  'orienteering problem'],\n",
       " 'CWE': ['common weakness enumeration',\n",
       "  'character - enhanced word embedding',\n",
       "  'chinese word embeddings'],\n",
       " 'MSR': ['minimum storage regenerating', 'mining software repositories'],\n",
       " 'PT': ['piecewise -testable',\n",
       "  'physical therapy',\n",
       "  'productive time',\n",
       "  'proof time'],\n",
       " 'PBRT': ['physically based renderer', 'physically based ray tracing'],\n",
       " 'NHS': ['national health service', \"nurses ' health study\"],\n",
       " 'LSC': ['leicester scientific corpus', 'long skip connections'],\n",
       " 'STL': ['single task learning',\n",
       "  'signal temporal logic',\n",
       "  'standard template library'],\n",
       " 'SBS': ['swedish blog sentences', 'small - cell base stations'],\n",
       " 'PAA': ['piecewise aggregation approximation', 'principal axis analysis'],\n",
       " 'CPS': ['common phone set', 'current population survey'],\n",
       " 'MVP': ['mitral valve prolapse', 'million veterans program'],\n",
       " 'MPB': ['matrix pair beamformer', 'modified poisson blending'],\n",
       " 'MIS': ['multiple importance sampling', 'maximal independent set'],\n",
       " 'LF': ['large faces', 'late fusion', 'line feeds'],\n",
       " 'SOC': ['security operations center',\n",
       "  'standard occupation classification',\n",
       "  'state of charge'],\n",
       " 'IVR': ['interactive voice response', 'immersive virtual reality'],\n",
       " 'IA': ['intent analyst',\n",
       "  'interval analysis',\n",
       "  'incremental approximation',\n",
       "  'interference alignment'],\n",
       " 'IFT': ['information foraging theory', 'information flow tracking'],\n",
       " 'ARS': ['augmented random search', 'addressee and response selection'],\n",
       " 'SCS': ['statistical compressed sensing',\n",
       "  'shortest common superstring',\n",
       "  'spoken conversational search',\n",
       "  'sub - carrier spacing'],\n",
       " 'SQA': ['semantic question answering', 'spoken question answering'],\n",
       " 'AWE': ['averaged word embeddings', 'address windowing extensions'],\n",
       " 'TAS': ['transverse abdominal section', 'transmit antenna selection'],\n",
       " 'IE': ['information extraction', 'intelligent element', 'integral equation'],\n",
       " 'CEM': ['causal effect map',\n",
       "  'circled entropy measurement',\n",
       "  'cross entropy methods'],\n",
       " 'UM': ['user model', 'upsampling module'],\n",
       " 'ILM': ['internal limiting membrane', 'information lifecycle management'],\n",
       " 'RGB': ['red , green , blue', 'red giant branch'],\n",
       " 'CSG': ['cumulative spectral gradient', 'cost sharing game'],\n",
       " 'PPC': ['peak power contract', 'pay per click'],\n",
       " 'EMG': ['eight medical grade', 'electromyograph'],\n",
       " 'DSP': ['digital signal processing', 'discrete sequence production'],\n",
       " 'MVF': ['maximum voice frequency', 'matching vector families'],\n",
       " 'FDA': [\"fisher 's discriminant analysis\", 'functional data analysis'],\n",
       " 'TDA': ['topological data analysis', 'targeted degree - based attack'],\n",
       " 'ERB': ['equivalent rectangular bandwidth', 'enhanced residual block'],\n",
       " 'EHS': ['enhanced hybrid simultaneous', 'enhanced hybrid swipt protocol'],\n",
       " 'SIR': ['sequential importance resampling', 'source to interferences ratio'],\n",
       " 'EMF': ['explicit matrix factorization',\n",
       "  'eclipse modeling framework',\n",
       "  'electromagnetic fields'],\n",
       " 'DLS': ['derandomized local search', 'depth - limited search'],\n",
       " 'AIDA': ['analytic imaging diagnostics arena',\n",
       "  'atomic , independent , declarative , and absolute'],\n",
       " 'NTM': ['neural turing machine', 'neural topic model'],\n",
       " 'DNS': ['domain name system', 'domain name service'],\n",
       " 'EF': ['expedited forwarding', 'ejection fraction', 'error feedback'],\n",
       " 'MTC': ['movie triplets corpus', 'machine type communications'],\n",
       " 'ISM': ['interactive skill modules', 'industrial , scientific and medical'],\n",
       " 'PTS': ['partial transmit sequences', 'public transportation system'],\n",
       " 'PDT': ['pulse discrete time', 'poisson delaunay tessellations'],\n",
       " 'PWM': ['pulse width modulation', 'partial weighted matching'],\n",
       " 'GCD': ['graphlet correlation distance', 'greatest common divisor'],\n",
       " 'CCG': ['calling contexts graphs',\n",
       "  'combinatory categorial grammar',\n",
       "  'chromatic correction gratings'],\n",
       " 'MCP': ['mean closest points', 'matern cluster process'],\n",
       " 'OR': ['optic radiation', 'operations research', 'opportunistic relaying'],\n",
       " 'DPN': ['dual path network', 'deep pyramid network'],\n",
       " 'LTT': ['lunar transfer trajectory', 'locally threshold testable'],\n",
       " 'MED': ['minimal edit distance', 'multimedia event detection'],\n",
       " 'FER': ['frame error rate', 'facial expression recognition'],\n",
       " 'AE': ['autoencoder',\n",
       "  'associative experiment',\n",
       "  'absolute error',\n",
       "  'answer extraction'],\n",
       " 'BPP': ['bits per pixel', 'binomial point process'],\n",
       " 'APD': ['average perpendicular distance', 'artifact pyramid decoding'],\n",
       " 'SEM': ['scanning electron microscopy',\n",
       "  'squared entropy measurement',\n",
       "  'simple event model'],\n",
       " 'PTM': ['point - to - multipoint', 'persistent turing machine'],\n",
       " 'CWT': ['continuous wavelet transform', 'complex wavelet transform'],\n",
       " 'TA': ['transmission antennas', 'threshold algorithm'],\n",
       " 'HOG': ['histogram of gradients', 'histogram of oriented gradient'],\n",
       " 'NCE': ['normalized cumulative entropy', 'noise contrastive estimation'],\n",
       " 'EP': ['entrance pupil',\n",
       "  'exponent parikh',\n",
       "  'efficient path',\n",
       "  'evolutionary programming',\n",
       "  'europarl'],\n",
       " 'COP': ['correlated orienteering problem',\n",
       "  'centralized optimization problem'],\n",
       " 'QMA': ['quantitative myotonia assessment', 'quantum merlin arthur'],\n",
       " 'MRT': ['minimum risk training', 'maximum ratio transmission'],\n",
       " 'VCC': ['vibrational coupled cluster', 'vehicular cloud computing'],\n",
       " 'RIC': ['restricted isometry constant', 'risk inflation criterion'],\n",
       " 'MV': ['mitral valve', 'memory vector'],\n",
       " 'OCT': ['optical coherence tomography', 'odd cycle transversal'],\n",
       " 'OA': ['open access', 'ocular artifacts', 'orthogonal array'],\n",
       " 'TBA': ['targeted betweenness - based attack', 'tailor based allocation'],\n",
       " 'IDE': ['integrated development environment',\n",
       "  'interprocedural distributive environment'],\n",
       " 'DCH': ['dynamic competition hypothesis',\n",
       "  'discriminant cross - modal hashing'],\n",
       " 'GAM': ['generalized additive models', 'generative adversarial metric'],\n",
       " 'TVD': ['total variation diminishing', 'threshold voltage defined'],\n",
       " 'RUM': ['random utility modelwe', 'random utility maximization'],\n",
       " 'TRI': ['temporal random indexing', 'toyota research institute'],\n",
       " 'QR': ['quantile regression', 'quadruple range'],\n",
       " 'MB': ['motion blurring', 'model - based', 'maximal biclique'],\n",
       " 'AFC': ['atomic function computation',\n",
       "  'automated fare collection(afc',\n",
       "  'automatic fact checking'],\n",
       " 'PSL': ['power service layer', 'probabilistic soft logic'],\n",
       " 'RPL': ['recurrent power law',\n",
       "  'routing protocol for low - power and lossy networks'],\n",
       " 'SPAM': ['subtractive pixel adjacency matrix',\n",
       "  'state preparation and measurement errors'],\n",
       " 'PSM': ['patient side manipulator', 'precoding - aided spatial modulation'],\n",
       " 'NI': ['new instances',\n",
       "  'neat image',\n",
       "  'national instruments',\n",
       "  'noun incorporation',\n",
       "  'network interface'],\n",
       " 'OPF': ['optimal power flow', 'optimal pareto front'],\n",
       " 'DTN': ['delay tolerant networks',\n",
       "  'domain transfer network',\n",
       "  'disruption tolerant networking'],\n",
       " 'PVI': ['parabolic variational inequality', 'perpendicular vegetation index'],\n",
       " 'ERR': ['expected reciprocal rank', 'exact recovery ratio'],\n",
       " 'HI': [\"hubert 's index\", 'histogram intersection'],\n",
       " 'TCA': ['temporal concept analysis', 'task component architecture'],\n",
       " 'NUC': ['normalized uniformity coefficient', 'next utterance classification'],\n",
       " 'OCM': ['oz computation model', 'original component manufacturers'],\n",
       " 'CCP': ['cross conformal prediction', 'convex - concave procedure'],\n",
       " 'JD': ['joint decoding', 'joint diagonalization'],\n",
       " 'LDE': ['local discriminant embedding', 'learnable dictionary encoding'],\n",
       " 'UC': ['universal composability', 'unit commitment'],\n",
       " 'CPD': ['concave points detection',\n",
       "  'coherent point drift',\n",
       "  'coal mine disaster'],\n",
       " 'HDT': ['header dictionary triple', 'header , dictionary , triples'],\n",
       " 'SSC': ['shapley share coefficient',\n",
       "  'sparse subspace clustering',\n",
       "  'similarity sensitive coding'],\n",
       " 'PDM': ['pulse density modulated', 'probability distribution matrix'],\n",
       " 'UCM': ['use case map', 'ultrametric contour map'],\n",
       " 'ZTD': ['zenith total delay', 'zenithal tropospheric delays'],\n",
       " 'FCA': ['formal concept analysis', 'forward capacity auctions'],\n",
       " 'BU': ['bottom - up', 'bandwidth units'],\n",
       " 'BCL': ['boolean coalgebraic logics', 'bilateral convolutional layers'],\n",
       " 'AQG': ['analyze questions generated', 'automatic question generation'],\n",
       " 'CSE': ['cumulative spectrum energy', 'common subexpression elimination'],\n",
       " 'MRD': ['mean rank difference', 'maximal ratio diversity'],\n",
       " 'NEM': ['net energy metering', 'new economy movement'],\n",
       " 'CLI': ['command line interface', 'cuneiform language identification'],\n",
       " 'MPCA': ['mobility prediction clustering algorithm',\n",
       "  'multi - linear principal components analysis'],\n",
       " 'QF': ['query fusion', 'quality factor', 'quadratic form'],\n",
       " 'BOA': ['butterfly optimization algorithm', 'bilevel optimization algorithm'],\n",
       " 'HP': ['high prr', 'hawkes processes'],\n",
       " 'CAA': ['civil aviation authority', 'clump assignment array'],\n",
       " 'ACI': ['adjacent channel interference',\n",
       "  'artificial collective intelligence'],\n",
       " 'OBS': ['optical burst - switched', 'optimal brain surgeon']}"
      ]
     },
     "metadata": {},
     "execution_count": 123
    }
   ],
   "source": [
    "diction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "50034\n"
     ]
    }
   ],
   "source": [
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6189\n"
     ]
    }
   ],
   "source": [
    "print(len(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6189\n"
     ]
    }
   ],
   "source": [
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(list_token):\n",
    "    return list(map(lambda x: x.lower(), list_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in train:\n",
    "    s[\"tokens\"] = normalize(s[\"tokens\"])\n",
    "    s[\"text\"] = \" \".join(s[\"tokens\"])\n",
    "    start_char_idx = 0\n",
    "    for i in range(0, s[\"acronym\"]):\n",
    "        start_char_idx += len(s[\"tokens\"][i])\n",
    "        start_char_idx += 1\n",
    "    s[\"start_char_idx\"] = start_char_idx\n",
    "    s[\"len_acronym\"] = len(s[\"tokens\"][s[\"acronym\"]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'acronym': 20,\n",
       " 'expansion': 'secrecy rate',\n",
       " 'id': 'TR-0',\n",
       " 'tokens': ['in',\n",
       "  'summary',\n",
       "  ',',\n",
       "  'it',\n",
       "  'is',\n",
       "  'evident',\n",
       "  'that',\n",
       "  'their',\n",
       "  'complexities',\n",
       "  'are',\n",
       "  'in',\n",
       "  'increasing',\n",
       "  'order',\n",
       "  ':',\n",
       "  'leakage',\n",
       "  '-',\n",
       "  'based',\n",
       "  ',',\n",
       "  'max',\n",
       "  '-',\n",
       "  'sr',\n",
       "  ',',\n",
       "  'and',\n",
       "  'generalized',\n",
       "  'edas',\n",
       "  '.'],\n",
       " 'text': 'in summary , it is evident that their complexities are in increasing order : leakage - based , max - sr , and generalized edas .',\n",
       " 'start_char_idx': 101,\n",
       " 'len_acronym': 2}"
      ]
     },
     "metadata": {},
     "execution_count": 129
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = diction[\"sr\".upper()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['stacked refinement',\n",
       " 'secrecy rate',\n",
       " 'segment representation',\n",
       " 'spatial resolution',\n",
       " 'success rate',\n",
       " 'super resolution',\n",
       " 'speech recognition',\n",
       " 'small resolution',\n",
       " 'strategic rationale',\n",
       " 'systematic review']"
      ]
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.remove(train[0][\"expansion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['stacked refinement',\n",
       " 'segment representation',\n",
       " 'spatial resolution',\n",
       " 'success rate',\n",
       " 'super resolution',\n",
       " 'speech recognition',\n",
       " 'small resolution',\n",
       " 'strategic rationale',\n",
       " 'systematic review']"
      ]
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['stacked refinement',\n",
       " 'secrecy rate',\n",
       " 'segment representation',\n",
       " 'spatial resolution',\n",
       " 'success rate',\n",
       " 'super resolution',\n",
       " 'speech recognition',\n",
       " 'small resolution',\n",
       " 'strategic rationale',\n",
       " 'systematic review']"
      ]
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": [
    "diction[\"sr\".upper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'acronym': 20,\n",
       " 'expansion': 'systematic review',\n",
       " 'id': 'TR-0',\n",
       " 'tokens': ['in',\n",
       "  'summary',\n",
       "  ',',\n",
       "  'it',\n",
       "  'is',\n",
       "  'evident',\n",
       "  'that',\n",
       "  'their',\n",
       "  'complexities',\n",
       "  'are',\n",
       "  'in',\n",
       "  'increasing',\n",
       "  'order',\n",
       "  ':',\n",
       "  'leakage',\n",
       "  '-',\n",
       "  'based',\n",
       "  ',',\n",
       "  'max',\n",
       "  '-',\n",
       "  'sr',\n",
       "  ',',\n",
       "  'and',\n",
       "  'generalized',\n",
       "  'edas',\n",
       "  '.'],\n",
       " 'text': 'in summary , it is evident that their complexities are in increasing order : leakage - based , max - sr , and generalized edas .',\n",
       " 'start_char_idx': 101,\n",
       " 'len_acronym': 2}"
      ]
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "tmp = 0\n",
    "for i in train:\n",
    "    try:\n",
    "        acronym = i[\"tokens\"][i[\"acronym\"]]\n",
    "        list_expansion_neg = diction[acronym.upper()].copy()\n",
    "        # print(list_expansion_neg)\n",
    "        list_expansion_neg.remove(i[\"expansion\"])\n",
    "        # print(list_expansion_neg)\n",
    "        if len(list_expansion_neg) > 1: \n",
    "            list_expansion_neg = random.sample(list_expansion_neg, random.randint(1,2))\n",
    "        for j in list_expansion_neg:\n",
    "            new_data.append(i.copy())\n",
    "            new_data[tmp][\"expansion\"] = j\n",
    "            tmp += 1\n",
    "    except: continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'acronym': 9,\n",
       " 'expansion': 'output constrained covariance',\n",
       " 'id': 'TR-9',\n",
       " 'tokens': ['to',\n",
       "  'our',\n",
       "  'knowledge',\n",
       "  ',',\n",
       "  'the',\n",
       "  'gain',\n",
       "  'design',\n",
       "  'problem',\n",
       "  'for',\n",
       "  'occ',\n",
       "  'has',\n",
       "  'not',\n",
       "  'been',\n",
       "  'completely',\n",
       "  'convexified',\n",
       "  'in',\n",
       "  'the',\n",
       "  'way',\n",
       "  'we',\n",
       "  'have',\n",
       "  'done',\n",
       "  'in',\n",
       "  'this',\n",
       "  'paper',\n",
       "  '.'],\n",
       " 'text': 'to our knowledge , the gain design problem for occ has not been completely convexified in the way we have done in this paper .',\n",
       " 'start_char_idx': 47,\n",
       " 'len_acronym': 3}"
      ]
     },
     "metadata": {},
     "execution_count": 152
    }
   ],
   "source": [
    "train[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "65516"
      ]
     },
     "metadata": {},
     "execution_count": 153
    }
   ],
   "source": [
    "len(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in dev:\n",
    "    s[\"tokens\"] = normalize(s[\"tokens\"])\n",
    "    s[\"text\"] = \" \".join(s[\"tokens\"])\n",
    "    start_char_idx = 0\n",
    "    for i in range(0, s[\"acronym\"]):\n",
    "        start_char_idx += len(s[\"tokens\"][i])\n",
    "        start_char_idx += 1\n",
    "    s[\"start_char_idx\"] = start_char_idx\n",
    "    s[\"len_acronym\"] = len(s[\"tokens\"][s[\"acronym\"]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample:\n",
    "    def __init__(self, tokenizer, expansion, context, start_char_idx, len_acronym, max_seq_lenght=384):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.expansion = expansion\n",
    "        self.context = context\n",
    "        self.start_char_idx = start_char_idx\n",
    "        self.len_acronym = len_acronym\n",
    "        self.max_seq_lenght = max_seq_lenght\n",
    "        self.skip = False\n",
    "        \n",
    "        self.start_token_idx = -1\n",
    "        self.end_token_idx = -1\n",
    "        \n",
    "    def preprocess(self):\n",
    "        tokenized_expansion = self.tokenizer.encode(self.expansion)\n",
    "        tokenized_context = self.tokenizer.encode(self.context)\n",
    "        \n",
    "        end_char_idx = self.start_char_idx + self.len_acronym\n",
    "        if end_char_idx >= len(self.context): \n",
    "            self.skip = True\n",
    "            return\n",
    "        \n",
    "        is_char_in_context = [0]*len(self.context)\n",
    "        for idx in range(self.start_char_idx, end_char_idx):\n",
    "            is_char_in_context[idx] = 1\n",
    "        \n",
    "        arc_token_idx  = []\n",
    "        for idx, (start, end) in enumerate(tokenized_context.offsets):\n",
    "            if sum(is_char_in_context[start:end]) > 0: arc_token_idx.append(idx)\n",
    "        if len(arc_token_idx) == 0:\n",
    "            self.skip = True\n",
    "            return\n",
    "        self.start_token_idx = arc_token_idx[0]\n",
    "        self.end_token_idx = arc_token_idx[-1]\n",
    "        \n",
    "        input_ids = tokenized_context.ids + tokenized_expansion.ids[1:]\n",
    "        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(tokenized_expansion.ids[1:])\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        \n",
    "        \n",
    "        padding_length = self.max_seq_lenght - len(input_ids)\n",
    "        if padding_length > 0:\n",
    "            input_ids = input_ids + ([0]* padding_length)\n",
    "            token_type_ids = token_type_ids + ([0]* padding_length)\n",
    "            attention_mask = attention_mask + ([0]* padding_length)\n",
    "        elif padding_length < 0:\n",
    "            self.skip = True\n",
    "            return\n",
    "        \n",
    "        self.input_ids = input_ids\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.attention_mask = attention_mask\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_examples(raw_data, desc, tokenizer):\n",
    "    p_bar = tqdm(total=len(raw_data), desc=desc,\n",
    "                 position=0, leave=True,\n",
    "                 file=sys.stdout, bar_format=\"{l_bar}%s{bar}%s{r_bar}\" % (Fore.BLUE, Fore.RESET))\n",
    "    examples = []\n",
    "    for item in raw_data:\n",
    "        expansion = item[\"expansion\"]\n",
    "        context = item[\"text\"]\n",
    "        start_char_idx = item[\"start_char_idx\"]\n",
    "        len_acronym = item[\"len_acronym\"]\n",
    "        example = Sample(tokenizer, expansion, context, start_char_idx, len_acronym)\n",
    "        example.preprocess()\n",
    "        examples.append(example)\n",
    "        p_bar.update(1)\n",
    "    p_bar.close()\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creating training points: 100%|\u001b[34m██████████\u001b[39m| 50034/50034 [00:14<00:00, 3459.60it/s]\n"
     ]
    }
   ],
   "source": [
    "examples = create_examples(train, \"Creating training points\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inputs_targets(examples):\n",
    "    dataset_dict = {\n",
    "        \"input_ids\": [],\n",
    "        \"token_type_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"start_token_idx\": [],\n",
    "        \"end_token_idx\": [],\n",
    "    }\n",
    "    for item in examples:\n",
    "        if item.skip is False:\n",
    "            for key in dataset_dict:\n",
    "                dataset_dict[key].append(getattr(item, key))\n",
    "    for key in dataset_dict:\n",
    "        dataset_dict[key] = np.array(dataset_dict[key])\n",
    "        \n",
    "    x = [dataset_dict[\"input_ids\"], dataset_dict[\"token_type_ids\"], dataset_dict[\"attention_mask\"]]\n",
    "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = create_inputs_targets(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(49880, 384)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "x_train[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}